{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51851844-f152-4b83-a827-6c2a79d60879",
   "metadata": {},
   "source": [
    "# MNIST Digit Recognizer\n",
    "\n",
    "**Authors: Clement, Calvin, Tilova**\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the very first project of the **Tequila Chicas**! We will be classifying images of hand written numbers to their corresponding digits. This project follows the guidelines and uses the data set provide from the Kaggle Competition [here](https://www.kaggle.com/competitions/digit-recognizer/overview). \n",
    "\n",
    "## Introduction  \n",
    "\n",
    "**MARKDOWN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1893221b-c4cc-4f1a-aa8b-dc54330d1067",
   "metadata": {},
   "source": [
    "<a id = 'toc'></a>\n",
    "    \n",
    "## Table of Contents\n",
    "---\n",
    "1. [Convolutional Neural Network](#CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72638dd3-5e64-498a-9640-a09b7863f28e",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4095d91-2b52-487a-9eef-2b05b9e64280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Train_Test_Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Progress bar from tqdm\n",
    "from tqdm.notebook import trange, tnrange, tqdm_notebook\n",
    "from tqdm.notebook import tnrange\n",
    "\n",
    "# ignores the filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0724e85-90ef-45c5-a600-cfa33931a7b0",
   "metadata": {},
   "source": [
    "<a id = 'CNN'></a>\n",
    "### 1. Convolutional Neural Network\n",
    "---\n",
    "Loading the test and train set CSVs files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eab5339-2a77-47e7-87c7-9d5250f03cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42000, 785), (28000, 784))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_test = pd.read_csv('../data/test.csv')\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de7b49-8bf2-403d-bfd4-e03f576e3b91",
   "metadata": {},
   "source": [
    "We need to set our independent (X) and dependent (y) variables as `numpy arrays` from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8b771da-8743-45c6-8645-e4994f14714d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784) (42000,)\n"
     ]
    }
   ],
   "source": [
    "X = df_train.iloc[:, 1:].to_numpy()\n",
    "y = df_train.iloc[:, 0].to_numpy()\n",
    "\n",
    "# sanity check\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab3f7f-9e5e-43ea-a2fe-85ae79deb1f8",
   "metadata": {},
   "source": [
    "We will perform a **train_test_split()** to split our dataset into train and validation sets.\n",
    "- Validation size of 25% of the data.\n",
    "- Stratify=y to make sure distribution of the classes remain the same in both training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fa45086-c761-4bbe-ab44-8f76f7f345ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31500, 784), (31500,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, stratify=y)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e465479-b796-4fc3-96c1-b29207974060",
   "metadata": {},
   "source": [
    "#### 1.1 Image Preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "**Steps**\n",
    "1. Scale the data\n",
    "2. Reshape the 1-D array into 2-D\n",
    "2. Convert 2-D array into Torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f989aed7-6bd4-4c02-8f9f-ef26663a4506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instantiate standard scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# fit and transform training\n",
    "X_train = ss.fit_transform(X_train)\n",
    "\n",
    "# ONLY transform X_val\n",
    "X_val = ss.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa42aa2a-61bf-4ab2-bd5d-36c43e6252da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31500, 28, 28) (10500, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# reshape training & validation\n",
    "X_train = np.array(X_train).reshape(-1, 28, 28)\n",
    "X_val = np.array(X_val).reshape(-1, 28, 28)\n",
    "\n",
    "# sanity check\n",
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27df9e9c-ff0c-451c-8157-4bffd48a0cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31500, 28, 28]) torch.Size([31500]) torch.Size([10500, 28, 28]) torch.Size([10500])\n"
     ]
    }
   ],
   "source": [
    "### To torch tensors ###\n",
    "# Independent Variables\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32, device='cuda')\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32, device='cuda')\n",
    "\n",
    "# Dependent Variable\n",
    "y_train = torch.tensor(y_train, dtype=torch.long, device='cuda')\n",
    "y_val = torch.tensor(y_val, dtype=torch.long, device='cuda')\n",
    "\n",
    "# Sanity Check\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baf33df",
   "metadata": {},
   "source": [
    "For a simple Convolutional Neural Network, we can have an architecture that looks like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfcdfd61-5804-4863-8bca-011e782dcebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Define the max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Define the fully connected (linear) layers\n",
    "        self.fc1 = nn.Linear(32 * 14 * 14, 128)  # Adjust the input size based on the image dimensions\n",
    "        self.fc2 = nn.Linear(128, 10)  # Output size is 10 for 10 classes (e.g., digits 0-9)\n",
    "        \n",
    "        # Define activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply first convolutional layer followed by ReLU activation and max pooling\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        \n",
    "        # Apply second convolutional layer followed by ReLU activation and max pooling\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        \n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(-1, 32 * 14 * 14)  # Adjust the size based on the output size of the convolutional layers\n",
    "        \n",
    "        # Apply fully connected layers followed by ReLU activation\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the SimpleCNN model\n",
    "model = SimpleCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fdaebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9d2a54c",
   "metadata": {},
   "source": [
    "### 1.2 Multi-Convolution Layering Explained\n",
    "\n",
    "Here we explain the layering for the CNN architecture below.\n",
    "\n",
    "Convolutional Neural Networks utilizes the idea of convolution to extract key features in objects such as images. For images, a kernel or window is slid through the image in order to extract key features. More specifically, if we have a 28x28 pixel image with 1 colour channel (greyscale), a convolution window can have a dimension of, let's say 3x3, and is slid across the one image channel where in each position the dot product of the window and the image pixels is computed to extract key features. Depending on whether or not padding is added, we will end up with an output feature map that may or may not have the same dimension as the input image, 28x28. \n",
    "> **Note 1**: The convolution we are using is 2D, i.e. a square, meaning it is applied per channel. Each (colour) channel has a convolution window. For greyscale images, we only have 1 convolution window, but for RGB images each colour channel has a separate window. Mind you, only having 1 convolution window for 1 channel is **not** the same as defining only one convolution layer in our architecture!\n",
    "\n",
    "> **Note 1.1**: Can think of `out_channels` in `nn.Conv2d` as the number of filters we want to look at for our images. In our 1x28x28 images, setting `out_channels=32`is essentially saying to look at our 28x28 image with 32 filters (Ex. edge detection, sharpen, blurring, etc.)\n",
    "\n",
    "> **Note 2**: 1x28x28 is the number of **features** we have for the image \n",
    "\n",
    "Now for colour images, let's say RGB, instead of 1 channel we have 3 now meaning our features becomes 3x28x28. For convolution, since we are working in 2D, we have 3 windows. One window for red, one window for green, and one window for blue. Each of these windows convolve over the 28x28 pixel image to extract the features. \n",
    "\n",
    "To calculate the number of input features we have for our fully-connected layer, we have to look at how many channels we ended up with after the convolution and how our image has changed size. By multiplying these two, we can get our input features for the fc-layer. For our case, we have 1x28x28 images, that is 1 channel (colour) image with 28x28 pixel. Looking at our convolution layers, we used maxpooling in each where the stride is set to 2. Since stride is how many pixels we are going to skip when we do pooling, with our 28x28 image after one maxpooling we will end up with a 14x14 image. Doing this two more times for 2 more layers we have 7x7 and then 3.5x3.5, we can't have half pixels though so using floor division we are left with 3x3. \n",
    "\n",
    "$$ \\begin{align}\n",
    "\\text{fc input features} &= \\text{layer 1} \\left(\\text{image size, } \\frac{28}{2}\\times\\frac{28}{2} = 14\\times14 \\right)\\times \\text{channels}\\\\\n",
    "&= \\text{layer 2} \\left( \\frac{14}{2}\\times\\frac{14}{2} = 7\\times7 \\right)\\times \\text{channels}\\\\\n",
    "&= \\text{layer 3} \\left( \\frac{7}{2}\\times\\frac{7}{2} = 3\\times3 \\text{ (floor division)}\\right)\\times \\text{channels}\\\\\n",
    "&= 3\\times3\\times128 \\text{ (last layer out channels is 128)}\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86990915-6e0c-474d-b5f0-f05b9231f563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultilayerCNN(nn.Module):\n",
    "    \"\"\"Basic multi-layer CNN architecture.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Define the main components of the network including the loss and optimizer.\"\"\"\n",
    "        super(MultilayerCNN, self).__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "\n",
    "            # Convolutional block 1\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1), # Setting padding = 1 to not lose information in the edges of the image\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.Dropout2d(0.1, inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Convolutional block 2\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.Dropout2d(0.1, inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Convolutional block 3\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        ) # the size of the flattened output tensor will be 64*128 // (2*2)\n",
    "        \n",
    "        # DROP OUT LAYER SETTINGS\n",
    "        self.dropout = nn.Dropout2d(0.3)\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "        nn.Linear(in_features=128*3*3, out_features=512),  # Corrected input size\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(in_features=512, out_features=10)  # Assuming 10 output classes\n",
    "        )\n",
    "\n",
    "        self.softmax_layer = nn.Softmax(dim=1)\n",
    "\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=.001, momentum=0.9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward pass.\"\"\"\n",
    "        # pass through the convolutional layers\n",
    "        x = self.conv_layer(x)\n",
    "\n",
    "        # flatten the output of the convolution\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # APPLYING DROP OUT\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # pass through the fully connected layers\n",
    "        x = self.fc_layer(x)\n",
    "\n",
    "        if not self.training:\n",
    "            x = self.softmax_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        Get hard class predictions from the\n",
    "        feature data\n",
    "        '''\n",
    "        predictions = self.forward(x)\n",
    "\n",
    "        # Find highest class logit, notice we don't need to convert to\n",
    "        # probabilities to do hard predictions, we can simply choose the\n",
    "        # highest values\n",
    "        hard_class_predictions = torch.argmax(predictions, dim=1)\n",
    "\n",
    "        return hard_class_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ff140008",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultilayerCNN(\n",
       "  (conv_layer): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (dropout): Dropout2d(p=0.3, inplace=False)\n",
       "  (fc_layer): Sequential(\n",
       "    (0): Linear(in_features=1152, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       "  (softmax_layer): Softmax(dim=1)\n",
       "  (cross_entropy_loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the model\n",
    "CNN_model = MultilayerCNN()\n",
    "#CNN_model = SimpleCNN()\n",
    "CNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da46087e-f4c4-4a8c-8c81-2b035be6fb67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RUN MODEL ON GPU\n",
    "device = torch.device(\"cuda\")\n",
    "CNN_model = CNN_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c61968",
   "metadata": {},
   "source": [
    " Let's try this on a sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2345d6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take the first training example\n",
    "sample = X_train[0]\n",
    "sample_label = y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0e38e62-60ce-4a6c-bd37-a654034d1a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(sample.shape, sample_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c469a9dd",
   "metadata": {},
   "source": [
    "PyTorch also takes in 4D tensors, usually in the form of $\\text{(batch_size, channels, height, width)}$. Since our images are only in pixels (28x28), we need to add the other dimensions by using `unsqueeze()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58d0ac01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.unsqueeze(0).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aead2add",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pass it through the model\n",
    "outputs = CNN_model(sample.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "10e30c3e-4295-4343-884b-7af2bb29e4f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3136, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the loss\n",
    "CNN_model.cross_entropy_loss(outputs, torch.tensor([sample_label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff5f5f8",
   "metadata": {},
   "source": [
    "Applying this to our full training set:\n",
    "- First let's look at the training and validation shapes\n",
    "- Apply unsqueeze() method to add a dimension after the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ec99286",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31500, 28, 28])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b103a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31500, 1, 28, 28])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81e10b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = X_train.unsqueeze(1)\n",
    "X_val = X_val.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147b813-29d9-4af7-837e-96c66a9df43e",
   "metadata": {},
   "source": [
    "Now that we have the appropriate dimensions for the model, we need to combine the independant (X) and dependant (y) variables into one tensor data so it can be fed into the `DataLoader` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19956a86-6b3e-44c1-a889-a48a63c27527",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#For both sets\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "12da96a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this iterator returns four training examples at a time so we will update the model after every 4 images\n",
    "train_dataloader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "\n",
    "# this iterator returns 1024 test examples at a time (for fast testing)\n",
    "val_dataloader = DataLoader(val_data, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b102e0cb-3651-45b1-a2ec-e18dabae5aca",
   "metadata": {},
   "source": [
    "To evaluate our model's performance, we can create a function that will return the model's accuracy:\n",
    "\n",
    "- Create 2 variables: correct and total\n",
    "- Go through the dataloader and find the predictions\n",
    "- Calculate how many labels the model predicted correctly by dividing correct and total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ced0a0d-3a90-4c7c-8202-82a6db3bbb20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre training accuracy: 98.72380952380952%\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(model, dataloader):\n",
    "    '''\n",
    "    Helper function to get classification accuracy for a model over the items in dataloader.\n",
    "    taken from: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "    '''\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "\n",
    "    # Go through all of the data\n",
    "    for batch in dataloader:\n",
    "        images, labels = batch\n",
    "\n",
    "        # Get the prediction of the net on the images\n",
    "        predicted = model.predict(images)\n",
    "\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Count those we got correct\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return(100 * correct / total)\n",
    "\n",
    "\n",
    "print(f\"Pre training accuracy: {get_accuracy(CNN_model, val_dataloader)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "77b31624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5242df98b54cc1a44d27465bd6fba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Total epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.4708 | Avg training accuracy: 85.8 | Avg test accuracy: 94.98\n",
      "Avg loss: 0.1 | Avg training accuracy: 97.23 | Avg test accuracy: 97.28\n",
      "Avg loss: 0.069 | Avg training accuracy: 98.11 | Avg test accuracy: 97.61\n",
      "Avg loss: 0.0513 | Avg training accuracy: 98.49 | Avg test accuracy: 98.16\n",
      "Avg loss: 0.0416 | Avg training accuracy: 98.82 | Avg test accuracy: 98.15\n",
      "Avg loss: 0.0333 | Avg training accuracy: 99.04 | Avg test accuracy: 98.23\n",
      "Avg loss: 0.0288 | Avg training accuracy: 99.18 | Avg test accuracy: 98.49\n",
      "Avg loss: 0.0253 | Avg training accuracy: 99.32 | Avg test accuracy: 98.43\n",
      "Avg loss: 0.0218 | Avg training accuracy: 99.33 | Avg test accuracy: 98.6\n",
      "Avg loss: 0.0207 | Avg training accuracy: 99.5 | Avg test accuracy: 98.57\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# re-initiliaze the model\n",
    "CNN_model = MultilayerCNN()\n",
    "\n",
    "### RUN MODEL ON GPU ###\n",
    "device = torch.device(\"cuda\")\n",
    "CNN_model = CNN_model.to(device)\n",
    "\n",
    "# MAIN EPOCH LOOP: the epochs are the number of times we loop through the entire training set.\n",
    "for epoch in tnrange(10, desc=\"Total epochs: \"):\n",
    "    \n",
    "    loss_sum = 0\n",
    "    acc_sum = 0\n",
    "    \n",
    "    # BATCH LOOP: loop over the data batches using the data loader \n",
    "    # if you don't have tqdm installed, just use this simpler for-loop instead\n",
    "    # for batch in train_dataloader: \n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # FORWARD PASS and loss calculation\n",
    "        outputs = CNN_model(inputs)\n",
    "        loss = CNN_model.cross_entropy_loss(outputs, labels)\n",
    "        \n",
    "        # BACKWARD PASS but zero the gradients first to delete the old ones\n",
    "        # as pytorch accumulates gradients by default\n",
    "        CNN_model.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # UPDATE: the model weights are updated\n",
    "        CNN_model.optimizer.step()\n",
    "        \n",
    "        # MONITORING: save loss and accuracy on the batch to track the training\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        # Get the prediction of the net on the images\n",
    "        predicted = CNN_model.predict(inputs)\n",
    "        acc_sum += (labels == predicted).sum().item() / labels.shape[0]\n",
    "        \n",
    "    \n",
    "    # print summary of training metrics\n",
    "    loss_avg = loss_sum / len(train_dataloader)\n",
    "    acc_avg = acc_sum / len(train_dataloader)\n",
    "    test_acc = get_accuracy(CNN_model, val_dataloader)\n",
    "    \n",
    "    print(f\"Avg loss: {np.round(loss_avg, 4)} | \"\\\n",
    "          f\"Avg training accuracy: {np.round(acc_avg*100, 2)} | \"\\\n",
    "          f\"Avg test accuracy: {np.round(test_acc, 2)}\")\n",
    "\n",
    "print('Finished Training') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f7e0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bccd55-0516-4aad-8e5d-41b1404af4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312aa899-9a3b-42ba-adad-a6cca5343f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb44a01-68c9-49dd-a0d1-6e753ab22153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70355219-d1bf-4462-80d1-8cdf12ddf302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a778aaa-3348-45d0-95b0-5f5d594bc241",
   "metadata": {},
   "source": [
    "### Predicting Test Cases (May Remove Later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6861ebef-822f-459c-a8bf-f9036945908a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reading test csv\n",
    "df_test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09631fd7-8a01-453a-806f-f65166072b84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# reshape training & validation\n",
    "X_test = np.array(df_test).reshape(-1, 28, 28)\n",
    "\n",
    "# sanity check\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d466edd0-d1c8-4d3e-8a04-758f0e89eb27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28000, 28, 28])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### To torch tensors ###\n",
    "# Independent Variables\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32, device='cuda')\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "844b92dc-79e5-47d0-8288-99e04167190b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = X_test.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b16bb7fd-cbf2-4c5f-b462-789256a42846",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28000, 1, 28, 28])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ff3dc4e9-21cc-4683-906c-0c714f243183",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m CNN_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "Cell \u001b[1;32mIn[49], line 66\u001b[0m, in \u001b[0;36mMultilayerCNN.predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     62\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    Get hard class predictions from the\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    feature data\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Find highest class logit, notice we don't need to convert to\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# probabilities to do hard predictions, we can simply choose the\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# highest values\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     hard_class_predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(predictions, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[49], line 45\u001b[0m, in \u001b[0;36mMultilayerCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Perform forward pass.\"\"\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# pass through the convolutional layers\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layer(x)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# flatten the output of the convolution\u001b[39;00m\n\u001b[0;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "CNN_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5612d8-2c84-45c0-89df-049349217096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f9bea0f1-3045-4354-8bb2-83744ad7f83e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare data loader\n",
    "# test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "# this iterator returns four training examples at a time so we will update the model after every 4 images\n",
    "test_dataloader = DataLoader(X_test, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1b9f72c7-8f48-4750-80dc-c12691a5b3e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Go through all of the data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m test_dataloader:\n\u001b[1;32m----> 3\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Get the prediction of the net on the images\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m CNN_model\u001b[38;5;241m.\u001b[39mpredict(images)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Go through all of the data\n",
    "for batch in test_dataloader:\n",
    "    images, labels = batch\n",
    "\n",
    "    # Get the prediction of the net on the images\n",
    "    predicted = CNN_model.predict(images)\n",
    "\n",
    "\n",
    "    total += labels.size(0)\n",
    "\n",
    "    # Count those we got correct\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9086dfc-ccde-4f52-80c4-677abb5c9959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
