{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51851844-f152-4b83-a827-6c2a79d60879",
   "metadata": {},
   "source": [
    "# MNIST Digit Recognizer\n",
    "\n",
    "**Authors: Clement, Calvin, Tilova**\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the very first project of the **Tequila Chicas**! We will be classifying images of hand written numbers to their corresponding digits. This project follows the guidelines and uses the data set provide from the Kaggle Competition [here](https://www.kaggle.com/competitions/digit-recognizer/overview). \n",
    "\n",
    "## Introduction  \n",
    "\n",
    "**MARKDOWN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1893221b-c4cc-4f1a-aa8b-dc54330d1067",
   "metadata": {},
   "source": [
    "<a id = 'toc'></a>\n",
    "    \n",
    "## Table of Contents\n",
    "---\n",
    "1. [Convolutional Neural Network](#CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72638dd3-5e64-498a-9640-a09b7863f28e",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4095d91-2b52-487a-9eef-2b05b9e64280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Train_Test_Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# ignores the filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0724e85-90ef-45c5-a600-cfa33931a7b0",
   "metadata": {},
   "source": [
    "<a id = 'CNN'></a>\n",
    "### 1. Convolutional Neural Network\n",
    "---\n",
    "Loading the test and train set CSVs files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eab5339-2a77-47e7-87c7-9d5250f03cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42000, 785), (28000, 784))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_test = pd.read_csv('../data/test.csv')\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de7b49-8bf2-403d-bfd4-e03f576e3b91",
   "metadata": {},
   "source": [
    "We need to set our independent (X) and dependent (y) variables as `numpy arrays` from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8b771da-8743-45c6-8645-e4994f14714d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784) (42000,)\n"
     ]
    }
   ],
   "source": [
    "X = df_train.iloc[:, 1:].to_numpy()\n",
    "y = df_train.iloc[:, 0].to_numpy()\n",
    "\n",
    "# sanity check\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab3f7f-9e5e-43ea-a2fe-85ae79deb1f8",
   "metadata": {},
   "source": [
    "We will perform a **train_test_split()** to split our dataset into train and validation sets.\n",
    "- Validation size of 25% of the data.\n",
    "- Stratify=y to make sure distribution of the classes remain the same in both training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5fa45086-c761-4bbe-ab44-8f76f7f345ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31500, 784), (31500,))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, stratify=y)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e465479-b796-4fc3-96c1-b29207974060",
   "metadata": {},
   "source": [
    "#### 1.1 Image Preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "**Steps**\n",
    "1. Scale the data\n",
    "2. Reshape the 1-D array into 2-D\n",
    "2. Convert 2-D array into Torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f989aed7-6bd4-4c02-8f9f-ef26663a4506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instantiate standard scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# fit and transform training\n",
    "X_train = ss.fit_transform(X_train)\n",
    "\n",
    "# ONLY transform X_val\n",
    "X_val = ss.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fa42aa2a-61bf-4ab2-bd5d-36c43e6252da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31500, 28, 28) (10500, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# reshape training & validation\n",
    "X_train = np.array(X_train).reshape(-1, 28, 28)\n",
    "X_val = np.array(X_val).reshape(-1, 28, 28)\n",
    "\n",
    "# sanity check\n",
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "27df9e9c-ff0c-451c-8157-4bffd48a0cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31500, 28, 28]) torch.Size([31500]) torch.Size([10500, 28, 28]) torch.Size([10500])\n"
     ]
    }
   ],
   "source": [
    "### To torch tensors ###\n",
    "# Independent Variables\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "# Dependent Variable\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Sanity Check\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baf33df",
   "metadata": {},
   "source": [
    "For a simple Convolutional Neural Network, we can have an architecture that looks like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bfcdfd61-5804-4863-8bca-011e782dcebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Define the max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Define the fully connected (linear) layers\n",
    "        self.fc1 = nn.Linear(32 * 14 * 14, 128)  # Adjust the input size based on the image dimensions\n",
    "        self.fc2 = nn.Linear(128, 10)  # Output size is 10 for 10 classes (e.g., digits 0-9)\n",
    "        \n",
    "        # Define activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply first convolutional layer followed by ReLU activation and max pooling\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        \n",
    "        # Apply second convolutional layer followed by ReLU activation and max pooling\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        \n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(-1, 32 * 14 * 14)  # Adjust the size based on the output size of the convolutional layers\n",
    "        \n",
    "        # Apply fully connected layers followed by ReLU activation\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the SimpleCNN model\n",
    "model = SimpleCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fdaebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9d2a54c",
   "metadata": {},
   "source": [
    "### 1.2 Multi-Convolution Layering Explained\n",
    "\n",
    "Here we explain the layering for the CNN architecture below.\n",
    "\n",
    "Convolutional Neural Networks utilizes the idea of convolution to extract key features in objects such as images. For images, a kernel or window is slid through the image in order to extract key features. More specifically, if we have a 28x28 pixel image with 1 colour channel (greyscale), a convolution window can have a dimension of, let's say 3x3, and is slid across the one image channel where in each position the dot product of the window and the image pixels is computed to extract key features. Depending on whether or not padding is added, we will end up with an output feature map that may or may not have the same dimension as the input image, 28x28. \n",
    "> **Note 1**: The convolution we are using is 2D, i.e. a square, meaning it is applied per channel. Each (colour) channel has a convolution window. For greyscale images, we only have 1 convolution window, but for RGB images each colour channel has a separate window. Mind you, only having 1 convolution window for 1 channel is **not** the same as defining only one convolution layer in our architecture!\n",
    "\n",
    "> **Note 1.1**: Can think of `out_channels` in `nn.Conv2d` as the number of filters we want to look at for our images. In our 1x28x28 images, setting `out_channels=32`is essentially saying to look at our 28x28 image with 32 filters\n",
    "\n",
    "> **Note 2**: 1x28x28 is the number of **features** we have for the image \n",
    "\n",
    "Now for colour images, let's say RGB, instead of 1 channel we have 3 now meaning our features becomes 3x28x28. For convolution, since we are working in 2D, we have 3 windows. One window for red, one window for green, and one window for blue. Each of these windows convolve over the 28x28 pixel image to extract the features. \n",
    "\n",
    "To calculate the number of input features we have for our fully-connected layer, we have to look at how many channels we ended up with after the convolution and how our image has changed size. By multiplying these two, we can get our input features for the fc-layer. For our case, we have 1x28x28 images, that is 1 channel (colour) image with 28x28 pixel. Looking at our convolution layers, we used maxpooling in each where the stride is set to 2. Since stride is how many pixels we are going to skip when we do pooling, with our 28x28 image after one maxpooling we will end up with a 14x14 image. Doing this two more times for 2 more layers we have 7x7 and then 3.5x3.5, we can't have half pixels though so using floor division we are left with 3x3. \n",
    "\n",
    "$$ \\begin{align}\n",
    "\\text{fc input features} &= \\text{layer 1} \\left(\\text{image size, } \\frac{28}{2}\\times\\frac{28}{2} = 14\\times14 \\right)\\times \\text{channels}\\\\\n",
    "&= \\text{layer 2} \\left( \\frac{14}{2}\\times\\frac{14}{2} = 7\\times7 \\right)\\times \\text{channels}\\\\\n",
    "&= \\text{layer 3} \\left( \\frac{7}{2}\\times\\frac{7}{2} = 3\\times3 \\text{ (floor division)}\\right)\\times \\text{channels}\\\\\n",
    "&= 3\\times3\\times128 \\text{ (last layer out channels is 128)}\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "86990915-6e0c-474d-b5f0-f05b9231f563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultilayerCNN(nn.Module):\n",
    "    \"\"\"Basic multi-layer CNN architecture.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Define the main components of the network including the loss and optimizer.\"\"\"\n",
    "        super(MultilayerCNN, self).__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "\n",
    "            # Convolutional block 1\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Convolutional block 2\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Convolutional block 3\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        ) # the size of the flattened output tensor will be 64*128 // (2*2)\n",
    "\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "        nn.Linear(in_features=128*3*3, out_features=512),  # Corrected input size\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(in_features=512, out_features=10)  # Assuming 10 output classes\n",
    "        )\n",
    "\n",
    "        self.softmax_layer = nn.Softmax(dim=1)\n",
    "\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=.001, momentum=0.9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward pass.\"\"\"\n",
    "        # pass through the convolutional layers\n",
    "        x = self.conv_layer(x)\n",
    "\n",
    "        # flatten the output of the convolution\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # pass through the fully connected layers\n",
    "        x = self.fc_layer(x)\n",
    "\n",
    "        if not self.training:\n",
    "            x = self.softmax_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        Get hard class predictions from the\n",
    "        feature data\n",
    "        '''\n",
    "        predictions = self.forward(x)\n",
    "\n",
    "        # Find highest class logit, notice we don't need to convert to\n",
    "        # probabilities to do hard predictions, we can simply choose the\n",
    "        # highest values\n",
    "        hard_class_predictions = torch.argmax(predictions, dim=1)\n",
    "\n",
    "        return hard_class_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ff140008",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultilayerCNN(\n",
       "  (conv_layer): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layer): Sequential(\n",
       "    (0): Linear(in_features=1152, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       "  (softmax_layer): Softmax(dim=1)\n",
       "  (cross_entropy_loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the model\n",
    "CNN_model = MultilayerCNN()\n",
    "#CNN_model = SimpleCNN()\n",
    "CNN_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c61968",
   "metadata": {},
   "source": [
    " Let's try this on a sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2345d6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take the first training example\n",
    "sample = X_train[0]\n",
    "sample_label = y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "50fdccae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c469a9dd",
   "metadata": {},
   "source": [
    "PyTorch also takes in 4D tensors, usually in the form of $\\text{(batch_size, channels, height, width)}$. Since our images are only in pixels (28x28), we need to add the other dimensions in using `unsqueeze()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "58d0ac01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.unsqueeze(0).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "aead2add",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pass it through the model\n",
    "outputs = CNN_model(sample.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "10e30c3e-4295-4343-884b-7af2bb29e4f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3142, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the loss\n",
    "CNN_model.cross_entropy_loss(outputs, torch.tensor([sample_label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff5f5f8",
   "metadata": {},
   "source": [
    "Applying this to our full training set:\n",
    "- **NOTE**: error seems to be coming from cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6ec99286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31500, 28, 28])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0b103a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31500, 1, 28, 28])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "81e10b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "46791866",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "12da96a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to combine our X_train and y_train tensor into one tensor as a training loader \n",
    "\n",
    "# this iterator returns four training examples at a time so we will update the model after every 4 images\n",
    "train_dataloader = torch.utils.data.DataLoader((X_train, y_train), batch_size=4, shuffle=True)\n",
    "\n",
    "# this iterator returns 1024 test examples at a time (for fast testing)\n",
    "val_dataloader = torch.utils.data.DataLoader((X_val, y_val), batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "bcec2d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "77b31624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f37c29ba7b4f5f86fcdaef074f899f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Total epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf0484c7976484fb309ecf3d16a7387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [31500] at entry 0 and [31500, 1, 28, 28] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[194], line 14\u001b[0m\n\u001b[1;32m      9\u001b[0m acc_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# BATCH LOOP: loop over the data batches using the data loader \u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# if you don't have tqdm installed, just use this simpler for-loop instead\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# for batch in train_dataloader: \u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm_notebook(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# get the inputs\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.8/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.8/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/utils/data/dataloader.py:345\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 345\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    348\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    349\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/utils/data/dataloader.py:385\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    384\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    387\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:47\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:55\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m         storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_new_shared(numel)\n\u001b[1;32m     54\u001b[0m         out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr_\u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring_\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     58\u001b[0m     elem \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [31500] at entry 0 and [31500, 1, 28, 28] at entry 1"
     ]
    }
   ],
   "source": [
    "# re-initiliaze the model\n",
    "CNN_model = MultilayerCNN()\n",
    "\n",
    "\n",
    "# MAIN EPOCH LOOP: the epochs are the number of times we loop through the entire training set.\n",
    "for epoch in tnrange(50, desc=\"Total epochs: \"):\n",
    "    \n",
    "    loss_sum = 0\n",
    "    acc_sum = 0\n",
    "    \n",
    "    # BATCH LOOP: loop over the data batches using the data loader \n",
    "    # if you don't have tqdm installed, just use this simpler for-loop instead\n",
    "    # for batch in train_dataloader: \n",
    "    for batch in tqdm_notebook(train_dataloader, desc=f\"Epoch {epoch}: \"):\n",
    "\n",
    "        # get the inputs\n",
    "        inputs, labels = batch\n",
    "        print(inputs.shape)\n",
    "        \n",
    "        # FORWARD PASS and loss calculation\n",
    "        outputs = CNN_model(inputs)\n",
    "        loss = CNN_model.cross_entropy_loss(outputs, labels)\n",
    "        \n",
    "        # BACKWARD PASS but zero the gradients first to delete the old ones\n",
    "        # as pytorch accumulates gradients by default\n",
    "        CNN_model.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # UPDATE: the model weights are updated\n",
    "        CNN_model.optimizer.step()\n",
    "        \n",
    "        # MONITORING: save loss and accuracy on the batch to track the training\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        # Get the prediction of the net on the images\n",
    "        predicted = CNN_model.predict(inputs)\n",
    "        acc_sum += (labels == predicted).sum().item() / labels.shape[0]\n",
    "        \n",
    "    \n",
    "    # print summary of training metrics\n",
    "    loss_avg = loss_sum / len(train_dataloader)\n",
    "    acc_avg = acc_sum / len(train_dataloader)\n",
    "    test_acc = get_accuracy(CNN_model, test_dataloader)\n",
    "    \n",
    "    print(f\"Avg loss: {np.round(loss_avg, 4)} | \"\\\n",
    "          f\"Avg training accuracy: {np.round(acc_avg*100, 2)} | \"\\\n",
    "          f\"Avg test accuracy: {np.round(test_acc, 2)}\")\n",
    "\n",
    "print('Finished Training') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f7e0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860338c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
