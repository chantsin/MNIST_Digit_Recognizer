{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51851844-f152-4b83-a827-6c2a79d60879",
   "metadata": {},
   "source": [
    "# MNIST Digit Recognizer\n",
    "\n",
    "**Authors: Clement, Calvin, Tilova**\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the very first project of the **Tequila Chicas**! We will be classifying images of hand written numbers to their corresponding digits. This project follows the guidelines and uses the data set provide from the Kaggle Competition [here](https://www.kaggle.com/competitions/digit-recognizer/overview). \n",
    "\n",
    "## Introduction  \n",
    "\n",
    "**MARKDOWN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1893221b-c4cc-4f1a-aa8b-dc54330d1067",
   "metadata": {},
   "source": [
    "<a id = 'toc'></a>\n",
    "    \n",
    "## Table of Contents\n",
    "---\n",
    "1. [Convolutional Neural Network](#CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72638dd3-5e64-498a-9640-a09b7863f28e",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4095d91-2b52-487a-9eef-2b05b9e64280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Train_Test_Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Progress bar from tqdm\n",
    "from tqdm.notebook import trange, tnrange, tqdm_notebook\n",
    "from tqdm.notebook import tnrange\n",
    "\n",
    "# ignores the filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0724e85-90ef-45c5-a600-cfa33931a7b0",
   "metadata": {},
   "source": [
    "<a id = 'CNN'></a>\n",
    "### 1. Convolutional Neural Network\n",
    "---\n",
    "Loading the test and train set CSVs files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eab5339-2a77-47e7-87c7-9d5250f03cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42000, 785), (28000, 784))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_test = pd.read_csv('../data/test.csv')\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de7b49-8bf2-403d-bfd4-e03f576e3b91",
   "metadata": {},
   "source": [
    "We need to set our independent (X) and dependent (y) variables as `numpy arrays` from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8b771da-8743-45c6-8645-e4994f14714d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784) (42000,)\n"
     ]
    }
   ],
   "source": [
    "X = df_train.iloc[:, 1:].to_numpy()\n",
    "y = df_train.iloc[:, 0].to_numpy()\n",
    "\n",
    "# sanity check\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab3f7f-9e5e-43ea-a2fe-85ae79deb1f8",
   "metadata": {},
   "source": [
    "We will perform a **train_test_split()** to split our dataset into train and validation sets.\n",
    "- Validation size of 25% of the data.\n",
    "- Stratify=y to make sure distribution of the classes remain the same in both training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fa45086-c761-4bbe-ab44-8f76f7f345ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31500, 784), (31500,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, stratify=y)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e465479-b796-4fc3-96c1-b29207974060",
   "metadata": {},
   "source": [
    "#### 1.1 Image Preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "**Steps**\n",
    "1. Scale the data\n",
    "2. Reshape the 1-D array into 2-D\n",
    "2. Convert 2-D array into Torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f989aed7-6bd4-4c02-8f9f-ef26663a4506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instantiate standard scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# fit and transform training\n",
    "X_train = ss.fit_transform(X_train)\n",
    "\n",
    "# ONLY transform X_val\n",
    "X_val = ss.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa42aa2a-61bf-4ab2-bd5d-36c43e6252da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31500, 28, 28) (10500, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# reshape training & validation\n",
    "X_train = np.array(X_train).reshape(-1, 28, 28)\n",
    "X_val = np.array(X_val).reshape(-1, 28, 28)\n",
    "\n",
    "# sanity check\n",
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27df9e9c-ff0c-451c-8157-4bffd48a0cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31500, 28, 28]) torch.Size([31500]) torch.Size([10500, 28, 28]) torch.Size([10500])\n"
     ]
    }
   ],
   "source": [
    "### To torch tensors ###\n",
    "# Independent Variables\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "# Dependent Variable\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Sanity Check\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baf33df",
   "metadata": {},
   "source": [
    "For a simple Convolutional Neural Network, we can have an architecture that looks like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfcdfd61-5804-4863-8bca-011e782dcebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Define the max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Define the fully connected (linear) layers\n",
    "        self.fc1 = nn.Linear(32 * 14 * 14, 128)  # Adjust the input size based on the image dimensions\n",
    "        self.fc2 = nn.Linear(128, 10)  # Output size is 10 for 10 classes (e.g., digits 0-9)\n",
    "        \n",
    "        # Define activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply first convolutional layer followed by ReLU activation and max pooling\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        \n",
    "        # Apply second convolutional layer followed by ReLU activation and max pooling\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        \n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(-1, 32 * 14 * 14)  # Adjust the size based on the output size of the convolutional layers\n",
    "        \n",
    "        # Apply fully connected layers followed by ReLU activation\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the SimpleCNN model\n",
    "model = SimpleCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fdaebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9d2a54c",
   "metadata": {},
   "source": [
    "### 1.2 Multi-Convolution Layering Explained\n",
    "\n",
    "Here we explain the layering for the CNN architecture below.\n",
    "\n",
    "Convolutional Neural Networks utilizes the idea of convolution to extract key features in objects such as images. For images, a kernel or window is slid through the image in order to extract key features. More specifically, if we have a 28x28 pixel image with 1 colour channel (greyscale), a convolution window can have a dimension of, let's say 3x3, and is slid across the one image channel where in each position the dot product of the window and the image pixels is computed to extract key features. Depending on whether or not padding is added, we will end up with an output feature map that may or may not have the same dimension as the input image, 28x28. \n",
    "> **Note 1**: The convolution we are using is 2D, i.e. a square, meaning it is applied per channel. Each (colour) channel has a convolution window. For greyscale images, we only have 1 convolution window, but for RGB images each colour channel has a separate window. Mind you, only having 1 convolution window for 1 channel is **not** the same as defining only one convolution layer in our architecture!\n",
    "\n",
    "> **Note 1.1**: Can think of `out_channels` in `nn.Conv2d` as the number of filters we want to look at for our images. In our 1x28x28 images, setting `out_channels=32`is essentially saying to look at our 28x28 image with 32 filters (Ex. edge detection, sharpen, blurring, etc.)\n",
    "\n",
    "> **Note 2**: 1x28x28 is the number of **features** we have for the image \n",
    "\n",
    "Now for colour images, let's say RGB, instead of 1 channel we have 3 now meaning our features becomes 3x28x28. For convolution, since we are working in 2D, we have 3 windows. One window for red, one window for green, and one window for blue. Each of these windows convolve over the 28x28 pixel image to extract the features. \n",
    "\n",
    "To calculate the number of input features we have for our fully-connected layer, we have to look at how many channels we ended up with after the convolution and how our image has changed size. By multiplying these two, we can get our input features for the fc-layer. For our case, we have 1x28x28 images, that is 1 channel (colour) image with 28x28 pixel. Looking at our convolution layers, we used maxpooling in each where the stride is set to 2. Since stride is how many pixels we are going to skip when we do pooling, with our 28x28 image after one maxpooling we will end up with a 14x14 image. Doing this two more times for 2 more layers we have 7x7 and then 3.5x3.5, we can't have half pixels though so using floor division we are left with 3x3. \n",
    "\n",
    "$$ \\begin{align}\n",
    "\\text{fc input features} &= \\text{layer 1} \\left(\\text{image size, } \\frac{28}{2}\\times\\frac{28}{2} = 14\\times14 \\right)\\times \\text{channels}\\\\\n",
    "&= \\text{layer 2} \\left( \\frac{14}{2}\\times\\frac{14}{2} = 7\\times7 \\right)\\times \\text{channels}\\\\\n",
    "&= \\text{layer 3} \\left( \\frac{7}{2}\\times\\frac{7}{2} = 3\\times3 \\text{ (floor division)}\\right)\\times \\text{channels}\\\\\n",
    "&= 3\\times3\\times128 \\text{ (last layer out channels is 128)}\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86990915-6e0c-474d-b5f0-f05b9231f563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultilayerCNN(nn.Module):\n",
    "    \"\"\"Basic multi-layer CNN architecture.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Define the main components of the network including the loss and optimizer.\"\"\"\n",
    "        super(MultilayerCNN, self).__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "\n",
    "            # Convolutional block 1\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1), # Setting padding = 1 to not lose information in the edges of the image\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Convolutional block 2\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # # Convolutional block 3\n",
    "            # nn.Conv2d(in_channels=64, out_channels=72, kernel_size=3, padding=1),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        ) # the size of the flattened output tensor will be 64*128 // (2*2)\n",
    "\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "        nn.Linear(in_features=64*7*7, out_features=192),  # Corrected input size\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(in_features=192, out_features=10)  # Assuming 10 output classes\n",
    "        )\n",
    "\n",
    "        self.softmax_layer = nn.Softmax(dim=1)\n",
    "\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=.001, momentum=0.9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward pass.\"\"\"\n",
    "        # pass through the convolutional layers\n",
    "        x = self.conv_layer(x)\n",
    "\n",
    "        # flatten the output of the convolution\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # pass through the fully connected layers\n",
    "        x = self.fc_layer(x)\n",
    "\n",
    "        if not self.training:\n",
    "            x = self.softmax_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        Get hard class predictions from the\n",
    "        feature data\n",
    "        '''\n",
    "        predictions = self.forward(x)\n",
    "\n",
    "        # Find highest class logit, notice we don't need to convert to\n",
    "        # probabilities to do hard predictions, we can simply choose the\n",
    "        # highest values\n",
    "        hard_class_predictions = torch.argmax(predictions, dim=1)\n",
    "\n",
    "        return hard_class_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff140008",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultilayerCNN(\n",
       "  (conv_layer): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layer): Sequential(\n",
       "    (0): Linear(in_features=3136, out_features=192, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=192, out_features=10, bias=True)\n",
       "  )\n",
       "  (softmax_layer): Softmax(dim=1)\n",
       "  (cross_entropy_loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the model\n",
    "CNN_model = MultilayerCNN()\n",
    "#CNN_model = SimpleCNN()\n",
    "CNN_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c61968",
   "metadata": {},
   "source": [
    " Let's try this on a sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2345d6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take the first training example\n",
    "sample = X_train[0]\n",
    "sample_label = y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0e38e62-60ce-4a6c-bd37-a654034d1a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(sample.shape, sample_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c469a9dd",
   "metadata": {},
   "source": [
    "PyTorch also takes in 4D tensors, usually in the form of $\\text{(batch_size, channels, height, width)}$. Since our images are only in pixels (28x28), we need to add the other dimensions by using `unsqueeze()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58d0ac01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.unsqueeze(0).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aead2add",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pass it through the model\n",
    "outputs = CNN_model(sample.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10e30c3e-4295-4343-884b-7af2bb29e4f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2904, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the loss\n",
    "CNN_model.cross_entropy_loss(outputs, torch.tensor([sample_label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff5f5f8",
   "metadata": {},
   "source": [
    "Applying this to our full training set:\n",
    "- First let's look at the training and validation shapes\n",
    "- Apply unsqueeze() method to add a dimension after the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ec99286",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31500, 28, 28])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b103a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31500, 1, 28, 28])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81e10b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = X_train.unsqueeze(1)\n",
    "X_val = X_val.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147b813-29d9-4af7-837e-96c66a9df43e",
   "metadata": {},
   "source": [
    "Now that we have the appropriate dimensions for the model, we need to combine the independant (X) and dependant (y) variables into one tensor data so it can be fed into the `DataLoader` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19956a86-6b3e-44c1-a889-a48a63c27527",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#For both sets\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12da96a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this iterator returns four training examples at a time so we will update the model after every 4 images\n",
    "train_dataloader = DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "\n",
    "# this iterator returns 1024 test examples at a time (for fast testing)\n",
    "val_dataloader = DataLoader(val_data, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b102e0cb-3651-45b1-a2ec-e18dabae5aca",
   "metadata": {},
   "source": [
    "To evaluate our model's performance, we can create a function that will return the model's accuracy:\n",
    "\n",
    "- Create 2 variables: correct and total\n",
    "- Go through the dataloader and find the predictions\n",
    "- Calculate how many labels the model predicted correctly by dividing correct and total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ced0a0d-3a90-4c7c-8202-82a6db3bbb20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre training accuracy: 17.047619047619047%\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(model, dataloader):\n",
    "    '''\n",
    "    Helper function to get classification accuracy for a model over the items in dataloader.\n",
    "    taken from: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "    '''\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "\n",
    "    # Go through all of the data\n",
    "    for batch in dataloader:\n",
    "        images, labels = batch\n",
    "\n",
    "        # Get the prediction of the net on the images\n",
    "        predicted = model.predict(images)\n",
    "\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Count those we got correct\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return(100 * correct / total)\n",
    "\n",
    "\n",
    "print(f\"Pre training accuracy: {get_accuracy(CNN_model, val_dataloader)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77b31624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98961b8de7fa4ee18b0c453a9c892831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Total epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.2331 | Avg training accuracy: 94.05 | Avg test accuracy: 97.48\n",
      "Avg loss: 0.0617 | Avg training accuracy: 98.59 | Avg test accuracy: 97.86\n",
      "Avg loss: 0.038 | Avg training accuracy: 99.11 | Avg test accuracy: 98.48\n",
      "Avg loss: 0.0272 | Avg training accuracy: 99.5 | Avg test accuracy: 98.37\n",
      "Avg loss: 0.0198 | Avg training accuracy: 99.68 | Avg test accuracy: 98.61\n",
      "Avg loss: 0.0137 | Avg training accuracy: 99.77 | Avg test accuracy: 98.61\n",
      "Avg loss: 0.0115 | Avg training accuracy: 99.81 | Avg test accuracy: 98.57\n",
      "Avg loss: 0.0082 | Avg training accuracy: 99.89 | Avg test accuracy: 98.81\n",
      "Avg loss: 0.005 | Avg training accuracy: 99.94 | Avg test accuracy: 98.62\n",
      "Avg loss: 0.0034 | Avg training accuracy: 99.97 | Avg test accuracy: 98.75\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# re-initiliaze the model\n",
    "CNN_model = MultilayerCNN()\n",
    "\n",
    "\n",
    "# MAIN EPOCH LOOP: the epochs are the number of times we loop through the entire training set.\n",
    "for epoch in tnrange(10, desc=\"Total epochs: \"):\n",
    "    \n",
    "    loss_sum = 0\n",
    "    acc_sum = 0\n",
    "    \n",
    "    # BATCH LOOP: loop over the data batches using the data loader \n",
    "    # if you don't have tqdm installed, just use this simpler for-loop instead\n",
    "    # for batch in train_dataloader: \n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # FORWARD PASS and loss calculation\n",
    "        outputs = CNN_model(inputs)\n",
    "        loss = CNN_model.cross_entropy_loss(outputs, labels)\n",
    "        \n",
    "        # BACKWARD PASS but zero the gradients first to delete the old ones\n",
    "        # as pytorch accumulates gradients by default\n",
    "        CNN_model.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # UPDATE: the model weights are updated\n",
    "        CNN_model.optimizer.step()\n",
    "        \n",
    "        # MONITORING: save loss and accuracy on the batch to track the training\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        # Get the prediction of the net on the images\n",
    "        predicted = CNN_model.predict(inputs)\n",
    "        acc_sum += (labels == predicted).sum().item() / labels.shape[0]\n",
    "        \n",
    "    \n",
    "    # print summary of training metrics\n",
    "    loss_avg = loss_sum / len(train_dataloader)\n",
    "    acc_avg = acc_sum / len(train_dataloader)\n",
    "    test_acc = get_accuracy(CNN_model, val_dataloader)\n",
    "    \n",
    "    print(f\"Avg loss: {np.round(loss_avg, 4)} | \"\\\n",
    "          f\"Avg training accuracy: {np.round(acc_avg*100, 2)} | \"\\\n",
    "          f\"Avg test accuracy: {np.round(test_acc, 2)}\")\n",
    "\n",
    "print('Finished Training') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f7e0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bccd55-0516-4aad-8e5d-41b1404af4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
